# Paper-Group-Reading
Repository for Miao's group weekly paper reading
## Fall 2023
| Date | Paper Title | Presenter | Notes |
| --------:| ----------------------------------------------------------------------- | ----------- | ---------- |
| 10.17 | [RPTQ: Reorder-based Post-training Quantization for Large Language Models][1] | Ming Wang | [Slide][2] |
| 10.24 | [Benign Overfitting in Two-layer Convolutional Neural Networks][3] | Jiarui Jiang | [Slide][4] |
| 10.31 | [PD-Quant: Post-Training Quantization based on Prediction Difference Metric][5] | Chao Zeng | [Slide][6] |
| 11.07 | [Real-World Image Super-Resolution as Multi-Task Learning][7] | Junpeng Jiang | [Slide][8] |
| 11.14 | [LLM-Pruner: On the Structural Pruning of Large Language Models][9] | Hongrong Cheng | [Slide][10] |
| 11.21 | [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers][11] | Jiaqi Zhao | [Slide][12] |
| 11.28 | [Denoising Diffusion Probabilistic Models][13] | Qianlong Xiang | [Slide][14] |
| 12.12 | [SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot][15] | Liyang Zheng | [Slide][16] |
| 12.19 | [Transformer-based model for symbolic regression via joint supervised learning ][17] | Ji Shi | [Slide][18] |
| 12.26 | [FINITE SCALAR QUANTIZATION:VQ-VAE MADE SIMPLE][19] | Lexiao Zou | [Slide][20] |




## Spring 2024
| Date | Paper Title | Presenter | Notes |
| -------:| ---------------------------------------------------------------------- | ---------- | ---------- |
| 1.16 | [Multisize Dataset Condensation][21] | Yanda Chen | [Slide][22] |
| 1.23 | [HOW DOES SEMI-SUPERVISED LEARNING WITH PSEUDO-LABELERS WORK? A CASE STUDY][23] | Jiarui Jiang | [Slide][24] |
| 1.30 | [Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling][25] | Chao Zeng | [Slide][26] |
| 3.5 | [ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models][27] | Ming Wang | [Slide][28] |
| 3.12 | [PB-LLM: Partially Binarized Large Language Models][29] | Jiaqi Zhao | [Slide][30] |
| 3.19 | [Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation][31] | Junpeng  Jiang | [Slide][32] |
| 3.26 | [Massive Activations in Large Language Models][33] | Lexiao Zou | [Slide][34] |
| 4.2 |  | Hongrong Cheng | [Slide][36] |

[1]:https://arxiv.org/pdf/2304.01089.pdf
[2]:Slides/23.10.17-wm.pdf
[3]:https://proceedings.neurips.cc/paper_files/paper/2022/file/a12c999be280372b157294e72a4bbc8b-Paper-Conference.pdf
[4]:Slides/23.10.24-jiarui.pdf
[5]:https://arxiv.org/pdf/2212.07048.pdf
[6]:Slides/23.10.31-chaozeng.pdf
[7]:https://openreview.net/pdf?id=8SCz56sUGP
[8]:Slides/23.11.07-junpeng.pdf
[9]:https://arxiv.org/pdf/2305.11627.pdf
[10]:Slides/23.11.14-chr.pdf
[11]:https://arxiv.org/pdf/2210.17323.pdf
[12]:Slides/23.11.21-zjq.pdf
[13]:https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html
[14]:Slides/23.11.28-xql.pdf
[15]:https://proceedings.mlr.press/v202/frantar23a.html
[16]:Slides/23.12.12-zly.pdf
[17]:https://openreview.net/forum?id=ULzyv9M1j5
[18]:Slides/23.12.19-sj.pdf
[19]:https://arxiv.org/pdf/2309.15505.pdf
[20]:Slides/23.12.26-zlx.pdf
[21]:https://openreview.net/pdf?id=FVhmnvqnsI
[22]:Slides/24.1.16-cyd.pdf
[23]:https://openreview.net/pdf?id=Dzmd-Cc8OI
[24]:Slides/24.1.23-jjr.pdf
[25]: https://aclanthology.org/2023.emnlp-main.102/
[26]: Slides/24.1.30-zc.pdf
[27]: https://openreview.net/forum?id=osoWxY8q2E
[28]: Slides/24.2.6-wm.pdf
[29]:https://arxiv.org/abs/2310.00034
[30]:Slides//24.3.10-zjq.pdf

[31]: https://arxiv.org/abs/2402.10491
[32]: Slides//24.3.19-jjp.pdf
[33]: https://arxiv.org/abs/2402.17762
[34]: Slides//24.3.26-zlx.pdf

[36]: Slides//24.4.2-chr.pdf

